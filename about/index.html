<!DOCTYPE html>
<html lang="en">

<head>
  <title>
   · Pavani Majety
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Pavani Majety">
<meta name="description" content="
  Pavani Majety
  
    
    Link to heading
  

Hi! I work as a Senior Deep Learning Engineer for Inference Frameworks and Compilers at NVIDIA. I primarily contribute to the  inference framework vLLM and the newly released compiler MLIR-TensorRT. I have previously worked at Mathworks as an Compiler Engineer on the Embedded Coder team and had a short stint at Trane as a Software Engineer before that.
I work on libraries that ensure that LLMs like GPTs, Llamas and Mixtrals give you the best perf when you run on NVIDIA GPUs from any inference framework. I use techniques like Mixed Precision Post Training Quantizations, integrating highly optimized attention implementations like Flash Attention, Flashinfer, etc, and write some CUDA code when required! A major part of ensuring that the perf is great, is to measure perf. I use tools like lm_eval, MLPerf, NSight Systems and NVBench for measuring various aspects of performance of a given model. For less mature spaces like DL Compilers, I also develop and use in-house tools for micro-benchmarking.">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Pavani Majety">
  <meta name="twitter:description" content="Pavani Majety Link to heading Hi! I work as a Senior Deep Learning Engineer for Inference Frameworks and Compilers at NVIDIA. I primarily contribute to the inference framework vLLM and the newly released compiler MLIR-TensorRT. I have previously worked at Mathworks as an Compiler Engineer on the Embedded Coder team and had a short stint at Trane as a Software Engineer before that.
I work on libraries that ensure that LLMs like GPTs, Llamas and Mixtrals give you the best perf when you run on NVIDIA GPUs from any inference framework. I use techniques like Mixed Precision Post Training Quantizations, integrating highly optimized attention implementations like Flash Attention, Flashinfer, etc, and write some CUDA code when required! A major part of ensuring that the perf is great, is to measure perf. I use tools like lm_eval, MLPerf, NSight Systems and NVBench for measuring various aspects of performance of a given model. For less mature spaces like DL Compilers, I also develop and use in-house tools for micro-benchmarking.">

<meta property="og:url" content="https://pavanimajety.github.io/about/">
  <meta property="og:site_name" content="Pavani Majety">
  <meta property="og:title" content="Pavani Majety">
  <meta property="og:description" content="Pavani Majety Link to heading Hi! I work as a Senior Deep Learning Engineer for Inference Frameworks and Compilers at NVIDIA. I primarily contribute to the inference framework vLLM and the newly released compiler MLIR-TensorRT. I have previously worked at Mathworks as an Compiler Engineer on the Embedded Coder team and had a short stint at Trane as a Software Engineer before that.
I work on libraries that ensure that LLMs like GPTs, Llamas and Mixtrals give you the best perf when you run on NVIDIA GPUs from any inference framework. I use techniques like Mixed Precision Post Training Quantizations, integrating highly optimized attention implementations like Flash Attention, Flashinfer, etc, and write some CUDA code when required! A major part of ensuring that the perf is great, is to measure perf. I use tools like lm_eval, MLPerf, NSight Systems and NVBench for measuring various aspects of performance of a given model. For less mature spaces like DL Compilers, I also develop and use in-house tools for micro-benchmarking.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2024-11-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-11-24T00:00:00+00:00">




<link rel="canonical" href="https://pavanimajety.github.io/about/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.aa5ef26fa979d6793724ae2dbd71efa94fd16cb1c5c7db3b6651f21f9892a5fd.css" integrity="sha256-ql7yb6l51nk3JK4tvXHvqU/RbLHFx9s7ZlHyH5iSpf0=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://pavanimajety.github.io/">
      Pavani Majety
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/resume">Resume</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container page">
  <article>
    <header>
      <h1 class="title">
        <a class="title-link" href="https://pavanimajety.github.io/about/">
          
        </a>
      </h1>
    </header>

    <h1 id="pavani-majety">
  Pavani Majety
  <a class="heading-link" href="#pavani-majety">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>Hi! I work as a Senior Deep Learning Engineer for Inference Frameworks and Compilers at NVIDIA. I primarily contribute to the  inference framework <a href="https://github.com/vllm-project/vllm"  class="external-link" target="_blank" rel="noopener">vLLM</a> and the newly released compiler <a href="https://github.com/NVIDIA/TensorRT-Incubator/tree/main/mlir-tensorrt"  class="external-link" target="_blank" rel="noopener">MLIR-TensorRT</a>. I have previously worked at Mathworks as an Compiler Engineer on the Embedded Coder team and had a short stint at Trane as a Software Engineer before that.</p>
<p>I work on libraries that ensure that LLMs like GPTs, Llamas and Mixtrals give you the best perf when you run on NVIDIA GPUs from any inference framework. I use techniques like Mixed Precision Post Training Quantizations, integrating highly optimized attention implementations like Flash Attention, Flashinfer, etc, and write some CUDA code when required! A major part of ensuring that the perf is great, is to measure perf. I use tools like lm_eval, MLPerf, NSight Systems and NVBench for measuring various aspects of performance of a given model. For less mature spaces like DL Compilers, I also develop and use in-house tools for micro-benchmarking.</p>
<p>I also enjoy developing compiler based solutions for compute and memory bound problems with Deep Learning Inference to alleviate the pain of hand optimizing GPU Kernels and writing application specific code on the framework side. With established compiler frameworks like MLIR, it has never been easier to tap into performance acceleration all while giving equal importance to scalability and portability.</p>
<p>I also like experimenting with Natural Language Processing, and Graph Machine Learning for a range of silly to hardcore engineering problems.</p>
<p>I love Taekwondo and it has become an integral part of who I am and how I present myself. I strive to abide by the tenets of Taekwondo: Courtesy(Ye Ui), Integrity(Yom Chi), Perseverance(In Nae), Self Control(Guk Gi) and Indomitable Spirit(Baekjul Bulgul) in all endeavors of my life inside and outside of a dojang.</p>
<p>Check out my GitHub contributions and projects!</p>
<p>Last Updated: Dec 14, 2024</p>

  </article>
</section>

  

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2021 -
    
    2024
     Pavani Majety 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
